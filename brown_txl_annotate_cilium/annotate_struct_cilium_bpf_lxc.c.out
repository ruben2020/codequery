#include <bpf/ctx/skb.h>
#include <bpf/api.h>
#include <ep_config.h>
#include <node_config.h>
#include <bpf/verifier.h>
#include <linux/icmpv6.h>
#define SKIP_SRV6_HANDLING
#define EVENT_SOURCE LXC_ID
#include "lib/tailcall.h"
#include "lib/common.h"
#include "lib/config.h"
#include "lib/maps.h"
#include "lib/arp.h"
#include "lib/edt.h"
#include "lib/qm.h"
#include "lib/ipv6.h"
#include "lib/ipv4.h"
#include "lib/icmp6.h"
#include "lib/eth.h"
#include "lib/dbg.h"
#include "lib/l3.h"
#include "lib/lxc.h"
#include "lib/identity.h"
#include "lib/policy.h"
#undef LB_SELECTION
#define LB_SELECTION LB_SELECTION_RANDOM
#include "lib/lb.h"
#include "lib/drop.h"
#include "lib/dbg.h"
#include "lib/trace.h"
#include "lib/csum.h"
#include "lib/egress_policies.h"
#include "lib/encap.h"
#include "lib/eps.h"
#include "lib/nat.h"
#include "lib/fib.h"
#include "lib/nodeport.h"
#include "lib/policy_log.h"
#if !defined(ENABLE_SOCKET_LB_FULL) || \
    defined(ENABLE_SOCKET_LB_HOST_ONLY) || \
    defined(ENABLE_L7_LB)
# define ENABLE_PER_PACKET_LB 1
#endif
#if defined(ENABLE_ARP_PASSTHROUGH) && defined(ENABLE_ARP_RESPONDER)
#error "Either ENABLE_ARP_PASSTHROUGH or ENABLE_ARP_RESPONDER can be defined"
#endif
#define HAVE_DIRECT_ACCESS_TO_MAP_VALUES \
    HAVE_PROG_TYPE_HELPER(sched_cls, bpf_fib_lookup)
#define TAIL_CT_LOOKUP4(ID, NAME, DIR, CONDITION, TARGET_ID, TARGET_NAME)	\
declare_tailcall_if(CONDITION, ID)						\
int NAME(struct __ctx_buff *ctx)						\
{										\
	struct ct_buffer4 ct_buffer = {};					\
	int l4_off, ret = CTX_ACT_OK;						\
	struct ipv4_ct_tuple *tuple;						\
	struct ct_state *ct_state;						\
	void *data, *data_end;							\
	struct iphdr *ip4;							\
	__u32 zero = 0;								\
										\
	ct_state = (struct ct_state *)&ct_buffer.ct_state;			\
	tuple = (struct ipv4_ct_tuple *)&ct_buffer.tuple;			\
										\
	if (!revalidate_data(ctx, &data, &data_end, &ip4))			\
		return DROP_INVALID;						\
										\
	tuple->nexthdr = ip4->protocol;						\
	tuple->daddr = ip4->daddr;						\
	tuple->saddr = ip4->saddr;						\
										\
	l4_off = ETH_HLEN + ipv4_hdrlen(ip4);					\
										\
	ct_buffer.ret = ct_lookup4(get_ct_map4(tuple), tuple, ctx, l4_off,	\
				   DIR, ct_state, &ct_buffer.monitor);		\
	if (ct_buffer.ret < 0)							\
		return ct_buffer.ret;						\
										\
	if (map_update_elem(&CT_TAIL_CALL_BUFFER4, &zero, &ct_buffer, 0) < 0)	\
		return DROP_INVALID_TC_BUFFER;					\
										\
	invoke_tailcall_if(CONDITION, TARGET_ID, TARGET_NAME);			\
	return ret;								\
}
#define TAIL_CT_LOOKUP6(ID, NAME, DIR, CONDITION, TARGET_ID, TARGET_NAME)	\
declare_tailcall_if(CONDITION, ID)						\
int NAME(struct __ctx_buff *ctx)						\
{										\
	int l4_off, ret = CTX_ACT_OK, hdrlen;					\
	struct ct_buffer6 ct_buffer = {};					\
	struct ipv6_ct_tuple *tuple;						\
	struct ct_state *ct_state;						\
	void *data, *data_end;							\
	struct ipv6hdr *ip6;							\
	__u32 zero = 0;								\
										\
	ct_state = (struct ct_state *)&ct_buffer.ct_state;			\
	tuple = (struct ipv6_ct_tuple *)&ct_buffer.tuple;			\
										\
	if (!revalidate_data(ctx, &data, &data_end, &ip6))			\
		return DROP_INVALID;						\
										\
	tuple->nexthdr = ip6->nexthdr;						\
	ipv6_addr_copy(&tuple->daddr, (union v6addr *)&ip6->daddr);		\
	ipv6_addr_copy(&tuple->saddr, (union v6addr *)&ip6->saddr);		\
										\
	hdrlen = ipv6_hdrlen(ctx, &tuple->nexthdr);				\
	if (hdrlen < 0)								\
		return hdrlen;							\
										\
	l4_off = ETH_HLEN + hdrlen;						\
										\
	ct_buffer.ret = ct_lookup6(get_ct_map6(tuple), tuple, ctx, l4_off,	\
				   DIR, ct_state, &ct_buffer.monitor);		\
	if (ct_buffer.ret < 0)							\
		return ct_buffer.ret;						\
										\
	if (map_update_elem(&CT_TAIL_CALL_BUFFER6, &zero, &ct_buffer, 0) < 0)	\
		return DROP_INVALID_TC_BUFFER;					\
										\
	invoke_tailcall_if(CONDITION, TARGET_ID, TARGET_NAME);			\
	return ret;								\
}
#if defined(ENABLE_IPV4) || defined(ENABLE_IPV6)

static __always_inline bool redirect_to_proxy (int verdict, enum ct_status status) {
    return is_defined (ENABLE_HOST_REDIRECT) && verdict > 0 && (status == CT_NEW || status == CT_ESTABLISHED || status == CT_REOPENED);
}
#endif
#ifdef ENABLE_CUSTOM_CALLS

static __always_inline int encode_custom_prog_meta (struct __ctx_buff *ctx, int ret, __u32 identity) {
    __u32 custom_meta = 0;
    if ((ret & 0xff) != ret)
        return -1;
    custom_meta |= (__u32) (ret & 0xff) << 24;
    custom_meta |= (identity & 0xffffff);
    ctx_store_meta (ctx, CB_CUSTOM_CALLS, custom_meta);
    return 0;
}
#endif
#ifdef ENABLE_IPV6

<struct>
struct ct_buffer6 {
    struct ipv6_ct_tuple tuple;
    struct ct_state ct_state;
    __u32 monitor;
    int ret;
};
</struct>

<struct>
struct {
    __uint (type, BPF_MAP_TYPE_PERCPU_ARRAY);
    __type (key, __u32);
    __type (value, struct ct_buffer6);
    __uint (max_entries, 1);
} CT_TAIL_CALL_BUFFER6 __section_maps_btf;
</struct>

static __always_inline int handle_ipv6_from_lxc (struct __ctx_buff *ctx, __u32 *dst_id) {
    struct ct_state ct_state_on_stack __maybe_unused, *ct_state, ct_state_new = {};
    struct ipv6_ct_tuple tuple_on_stack __maybe_unused, *tuple;
#ifdef ENABLE_ROUTING
    union macaddr router_mac = NODE_MAC;
#endif
    struct ct_buffer6 *ct_buffer;
    void *data, *data_end;
    struct ipv6hdr *ip6;
    int ret, verdict = 0, l4_off, hdrlen, zero = 0;
    struct trace_ctx trace = {
        .reason = TRACE_REASON_UNKNOWN,
        .monitor = 0,
    };
    __u32 __maybe_unused tunnel_endpoint = 0;
    __u8 __maybe_unused encrypt_key = 0;
    enum ct_status ct_status;
    bool hairpin_flow = false;
    __u8 policy_match_type = POLICY_MATCH_NONE;
    __u8 audited = 0;
    bool __maybe_unused dst_remote_ep = false;
    __u16 proxy_port = 0;
    bool from_l7lb = false;
    bool emit_policy_verdict = true;
    if (!revalidate_data (ctx, &data, &data_end, &ip6))
        return DROP_INVALID;
    if (1) {
        const union v6addr *daddr = (union v6addr *) &ip6->daddr;
        struct remote_endpoint_info *info;
        info = lookup_ip6_remote_endpoint (daddr);
        if (info && info->sec_label) {
            *dst_id = info->sec_label;
            tunnel_endpoint = info->tunnel_endpoint;
            encrypt_key = get_min_encrypt_key (info->key);
#ifdef ENABLE_WIREGUARD
            if (info->tunnel_endpoint != 0 && !identity_is_node (info->sec_label))
                dst_remote_ep = true;
#endif /* ENABLE_WIREGUARD */
        }
        else {
            *dst_id = WORLD_ID;
        }
        cilium_dbg (ctx, info ? DBG_IP_ID_MAP_SUCCEED6 : DBG_IP_ID_MAP_FAILED6, daddr -> p4, * dst_id);
    }
#ifdef ENABLE_PER_PACKET_LB
#if !defined(DEBUG) && defined(TUNNEL_MODE)
    if (!revalidate_data (ctx, &data, &data_end, &ip6))
        return DROP_INVALID;
#endif
    lb6_ctx_restore_state (ctx, & ct_state_new, & proxy_port);
#endif /* ENABLE_PER_PACKET_LB */
    ct_buffer = map_lookup_elem (&CT_TAIL_CALL_BUFFER6, &zero);
    if (!ct_buffer)
        return DROP_INVALID_TC_BUFFER;
    if (ct_buffer->tuple.saddr.d1 == 0 && ct_buffer->tuple.saddr.d2 == 0)
        return DROP_INVALID_TC_BUFFER;
#if HAVE_DIRECT_ACCESS_TO_MAP_VALUES
    tuple = (struct ipv6_ct_tuple *) &ct_buffer->tuple;
    ct_state = (struct ct_state *) &ct_buffer->ct_state;
#else
    memcpy (& tuple_on_stack, & ct_buffer -> tuple, sizeof (tuple_on_stack));
    tuple = &tuple_on_stack;
    memcpy (& ct_state_on_stack, & ct_buffer -> ct_state, sizeof (ct_state_on_stack));
    ct_state = &ct_state_on_stack;
#endif /* HAVE_DIRECT_ACCESS_TO_MAP_VALUES */
    trace.monitor = ct_buffer->monitor;
    ret = ct_buffer->ret;
    ct_status = (enum ct_status) ret;
    trace.reason = (enum trace_reason) ret;
#if defined(ENABLE_L7_LB)
    if (proxy_port > 0) {
        cilium_dbg3 (ctx, DBG_L7_LB, tuple -> daddr.p4, tuple -> saddr.p4, bpf_ntohs (proxy_port));
        verdict = proxy_port;
        emit_policy_verdict = false;
        goto skip_policy_enforcement;
    }
#endif /* ENABLE_L7_LB */
    if ((ct_status == CT_REPLY || ct_status == CT_RELATED) && ct_state->proxy_redirect) {
        return ctx_redirect_to_proxy6 (ctx, tuple, 0, false);
    }
    if (hairpin_flow) {
        emit_policy_verdict = false;
        goto skip_policy_enforcement;
    }
    verdict = policy_can_egress6 (ctx, tuple, SECLABEL, *dst_id, &policy_match_type, &audited);
    if (ct_status != CT_REPLY && ct_status != CT_RELATED && verdict < 0) {
        send_policy_verdict_notify (ctx, * dst_id, tuple -> dport, tuple -> nexthdr, POLICY_EGRESS, 1, verdict, policy_match_type, audited);
        return verdict;
    }
skip_policy_enforcement :
#if defined(ENABLE_L7_LB)
    from_l7lb = ctx_load_meta (ctx, CB_FROM_HOST) == FROM_HOST_L7_LB;
#endif
    switch (ct_status) {
    case CT_NEW :
        if (emit_policy_verdict)
            send_policy_verdict_notify (ctx, *dst_id, tuple->dport, tuple->nexthdr, POLICY_EGRESS, 1, verdict, policy_match_type, audited);
    ct_recreate6 :
        ct_state_new.src_sec_id = SECLABEL;
        ret = ct_create6 (get_ct_map6 (tuple), &CT_MAP_ANY6, tuple, ctx, CT_EGRESS, &ct_state_new, verdict > 0, from_l7lb);
        if (IS_ERR (ret))
            return ret;
        trace.monitor = TRACE_PAYLOAD_LEN;
        break;
    case CT_REOPENED :
        if (emit_policy_verdict)
            send_policy_verdict_notify (ctx, *dst_id, tuple->dport, tuple->nexthdr, POLICY_EGRESS, 1, verdict, policy_match_type, audited);
    case CT_ESTABLISHED :
        if (unlikely (ct_state->rev_nat_index != ct_state_new.rev_nat_index))
            goto ct_recreate6;
        break;
    case CT_RELATED :
    case CT_REPLY :
        policy_mark_skip (ctx);
        hdrlen = ipv6_hdrlen (ctx, &tuple->nexthdr);
        if (hdrlen < 0)
            return hdrlen;
        l4_off = ETH_HLEN + hdrlen;
#ifdef ENABLE_NODEPORT
# ifdef ENABLE_DSR
        if (ct_state->dsr) {
            ret = xlate_dsr_v6 (ctx, tuple, l4_off);
            if (ret != 0)
                return ret;
        }
        else
# endif /* ENABLE_DSR */
            if (ct_state->node_port) {
                send_trace_notify (ctx, TRACE_TO_NETWORK, SECLABEL, * dst_id, 0, 0, trace.reason, trace.monitor);
                ctx->tc_index |= TC_INDEX_F_SKIP_RECIRCULATION;
                ep_tail_call (ctx, CILIUM_CALL_IPV6_NODEPORT_REVNAT);
                return DROP_MISSED_TAIL_CALL;
            }
#endif /* ENABLE_NODEPORT */
        if (ct_state->rev_nat_index) {
            struct csum_offset csum_off = {};
            csum_l4_offset_and_flags (tuple -> nexthdr, & csum_off);
            ret = lb6_rev_nat (ctx, l4_off, &csum_off, ct_state->rev_nat_index, tuple, 0);
            if (IS_ERR (ret))
                return ret;
            policy_mark_skip (ctx);
        }
        break;
    default :
        return DROP_UNKNOWN_CT;
    }
    hairpin_flow |= ct_state->loopback;
    if (!from_l7lb && redirect_to_proxy (verdict, ct_status)) {
        proxy_port = (__u16) verdict;
        send_trace_notify (ctx, TRACE_TO_PROXY, SECLABEL, 0, bpf_ntohs (proxy_port), 0, trace.reason, trace.monitor);
        return ctx_redirect_to_proxy6 (ctx, tuple, proxy_port, false);
    }
    if (!revalidate_data (ctx, &data, &data_end, &ip6))
        return DROP_INVALID;
    if (is_defined (ENABLE_ROUTING) || hairpin_flow) {
        struct endpoint_info *ep;
        ep = lookup_ip6_endpoint (ip6);
        if (ep) {
#ifdef ENABLE_ROUTING
            if (ep->flags & ENDPOINT_F_HOST) {
#ifdef HOST_IFINDEX
                goto to_host;
#else
                return DROP_HOST_UNREACHABLE;
#endif
            }
#endif /* ENABLE_ROUTING */
            policy_clear_mark (ctx);
            return ipv6_local_delivery (ctx, ETH_HLEN, SECLABEL, ep, METRIC_EGRESS, from_l7lb);
        }
    }
#if defined(ENABLE_HOST_FIREWALL) && !defined(ENABLE_ROUTING)
    if (*dst_id == HOST_ID) {
        ctx_store_meta (ctx, CB_FROM_HOST, 0);
        tail_call_static (ctx, & POLICY_CALL_MAP, HOST_EP_ID);
        return DROP_MISSED_TAIL_CALL;
    }
#endif /* ENABLE_HOST_FIREWALL && !ENABLE_ROUTING */
#ifdef TUNNEL_MODE
# ifdef ENABLE_WIREGUARD
    if (!dst_remote_ep)
# endif /* ENABLE_WIREGUARD */
        {
            struct endpoint_key key = {};
            union v6addr *daddr = (union v6addr *) &ip6->daddr;
            key.ip6.p1 = daddr->p1;
            key.ip6.p2 = daddr->p2;
            key.ip6.p3 = daddr->p3;
            key.family = ENDPOINT_KEY_IPV6;
            ret = encap_and_redirect_lxc (ctx, tunnel_endpoint, encrypt_key, &key, SECLABEL, &trace);
            if (ret == IPSEC_ENDPOINT)
                goto encrypt_to_stack;
            else if (ret != DROP_NO_TUNNEL_ENDPOINT)
                return ret;
        }
#endif
    if (is_defined (ENABLE_HOST_ROUTING))
        return redirect_direct_v6 (ctx, ETH_HLEN, ip6);
    goto pass_to_stack;
#ifdef ENABLE_ROUTING
to_host :
    if (is_defined (ENABLE_HOST_FIREWALL) && *dst_id == HOST_ID) {
        send_trace_notify (ctx, TRACE_TO_HOST, SECLABEL, HOST_ID, 0, HOST_IFINDEX, trace.reason, trace.monitor);
        return ctx_redirect (ctx, HOST_IFINDEX, BPF_F_INGRESS);
    }
#endif
pass_to_stack :
#ifdef ENABLE_ROUTING
    ret = ipv6_l3 (ctx, ETH_HLEN, NULL, (__u8 *) &router_mac.addr, METRIC_EGRESS);
    if (unlikely (ret != CTX_ACT_OK))
        return ret;
#endif
    if (ipv6_store_flowlabel (ctx, ETH_HLEN, SECLABEL_NB) < 0)
        return DROP_WRITE_ERROR;
#ifdef ENABLE_WIREGUARD
    if (dst_remote_ep)
        set_encrypt_mark (ctx);
    else
#elif !defined(TUNNEL_MODE)
# ifdef ENABLE_IPSEC
        if (encrypt_key && tunnel_endpoint) {
            set_encrypt_key_mark (ctx, encrypt_key);
#  ifdef IP_POOLS
            set_encrypt_dip (ctx, tunnel_endpoint);
#  endif /* IP_POOLS */
#  ifdef ENABLE_IDENTITY_MARK
            set_identity_mark (ctx, SECLABEL);
#  endif /* ENABLE_IDENTITY_MARK */
        }
        else
# endif /* ENABLE_IPSEC */
#endif /* ENABLE_WIREGUARD */
            {
#ifdef ENABLE_IDENTITY_MARK
                ctx->mark |= MARK_MAGIC_IDENTITY;
                set_identity_mark (ctx, SECLABEL);
#endif
            }
#ifdef TUNNEL_MODE
encrypt_to_stack :
#endif
    send_trace_notify (ctx, TRACE_TO_STACK, SECLABEL, * dst_id, 0, 0, trace.reason, trace.monitor);
    cilium_dbg_capture (ctx, DBG_CAPTURE_DELIVERY, 0);
    return CTX_ACT_OK;
}

declare_tailcall_if (is_defined (ENABLE_PER_PACKET_LB), CILIUM_CALL_IPV6_FROM_LXC_CONT)
int tail_handle_ipv6_cont (struct __ctx_buff *ctx) {
    __u32 dst_id = 0;
    int ret = handle_ipv6_from_lxc (ctx, &dst_id);
    if (IS_ERR (ret))
        return send_drop_notify (ctx, SECLABEL, dst_id, 0, ret, CTX_ACT_DROP, METRIC_EGRESS);
#ifdef ENABLE_CUSTOM_CALLS
    if (!encode_custom_prog_meta (ctx, ret, dst_id)) {
        tail_call_static (ctx, & CUSTOM_CALLS_MAP, CUSTOM_CALLS_IDX_IPV6_EGRESS);
        update_metrics (ctx_full_len (ctx), METRIC_EGRESS, REASON_MISSED_CUSTOM_CALL);
    }
#endif
    return ret;
}

TAIL_CT_LOOKUP6 (CILIUM_CALL_IPV6_CT_EGRESS, tail_ipv6_ct_egress, CT_EGRESS, is_defined (ENABLE_PER_PACKET_LB), CILIUM_CALL_IPV6_FROM_LXC_CONT, tail_handle_ipv6_cont)
static __always_inline int __tail_handle_ipv6 (struct __ctx_buff *ctx) {
    void *data, *data_end;
    struct ipv6hdr *ip6;
    int ret;
    if (!revalidate_data_pull (ctx, &data, &data_end, &ip6))
        return DROP_INVALID;
    if (unlikely (ip6->nexthdr == IPPROTO_ICMPV6)) {
        if (data + sizeof (*ip6) + ETH_HLEN + sizeof (struct icmp6hdr) > data_end)
            return DROP_INVALID;
        ret = icmp6_handle (ctx, ETH_HLEN, ip6, METRIC_EGRESS);
        if (IS_ERR (ret))
            return ret;
    }
    if (unlikely (!is_valid_lxc_src_ip (ip6)))
        return DROP_INVALID_SIP;
#ifdef ENABLE_PER_PACKET_LB
    {
        struct ipv6_ct_tuple tuple = {};
        struct csum_offset csum_off = {};
        struct ct_state ct_state_new = {};
        struct lb6_service *svc;
        struct lb6_key key = {};
        __u16 proxy_port = 0;
        int l4_off, hdrlen;
        tuple.nexthdr = ip6->nexthdr;
        ipv6_addr_copy (& tuple.daddr, (union v6addr *) & ip6 -> daddr);
        ipv6_addr_copy (& tuple.saddr, (union v6addr *) & ip6 -> saddr);
        hdrlen = ipv6_hdrlen (ctx, &tuple.nexthdr);
        if (hdrlen < 0)
            return hdrlen;
        l4_off = ETH_HLEN + hdrlen;
        ret = lb6_extract_key (ctx, &tuple, l4_off, &key, &csum_off, CT_EGRESS);
        if (IS_ERR (ret)) {
            if (ret == DROP_NO_SERVICE || ret == DROP_UNKNOWN_L4)
                goto skip_service_lookup;
            else
                return ret;
        }
        svc = lb6_lookup_service (&key, is_defined (ENABLE_NODEPORT));
        if (svc) {
#if defined(ENABLE_L7_LB)
            if (lb6_svc_is_l7loadbalancer (svc)) {
                proxy_port = (__u16) svc->l7_lb_proxy_port;
                goto skip_service_lookup;
            }
#endif /* ENABLE_L7_LB */
            ret = lb6_local (get_ct_map6 (&tuple), ctx, ETH_HLEN, l4_off, &csum_off, &key, &tuple, svc, &ct_state_new, false);
            if (IS_ERR (ret))
                return ret;
        }
    skip_service_lookup :
        lb6_ctx_store_state (ctx, &ct_state_new, proxy_port);
    }
#endif /* ENABLE_PER_PACKET_LB */
    invoke_tailcall_if (is_defined (ENABLE_PER_PACKET_LB), CILIUM_CALL_IPV6_CT_EGRESS, tail_ipv6_ct_egress);
    return ret;
}

__section_tail (CILIUM_MAP_CALLS, CILIUM_CALL_IPV6_FROM_LXC)
int tail_handle_ipv6 (struct __ctx_buff *ctx) {
    int ret = __tail_handle_ipv6 (ctx);
    if (IS_ERR (ret))
        return send_drop_notify_error (ctx, SECLABEL, ret, CTX_ACT_DROP, METRIC_EGRESS);
    return ret;
}
#endif /* ENABLE_IPV6 */
#ifdef ENABLE_IPV4

<struct>
struct ct_buffer4 {
    struct ipv4_ct_tuple tuple;
    struct ct_state ct_state;
    __u32 monitor;
    int ret;
};
</struct>

<struct>
struct {
    __uint (type, BPF_MAP_TYPE_PERCPU_ARRAY);
    __type (key, __u32);
    __type (value, struct ct_buffer4);
    __uint (max_entries, 1);
} CT_TAIL_CALL_BUFFER4 __section_maps_btf;
</struct>

static __always_inline int handle_ipv4_from_lxc (struct __ctx_buff *ctx, __u32 *dst_id) {
    struct ct_state ct_state_on_stack __maybe_unused, *ct_state, ct_state_new = {};
    struct ipv4_ct_tuple tuple_on_stack __maybe_unused, *tuple;
#ifdef ENABLE_ROUTING
    union macaddr router_mac = NODE_MAC;
#endif
    void *data, *data_end;
    struct iphdr *ip4;
    int ret, verdict = 0, l4_off;
    struct trace_ctx trace = {
        .reason = TRACE_REASON_UNKNOWN,
        .monitor = 0,
    };
    __u32 __maybe_unused tunnel_endpoint = 0, zero = 0;
    __u8 __maybe_unused encrypt_key = 0;
    bool hairpin_flow = false;
    __u8 policy_match_type = POLICY_MATCH_NONE;
    struct ct_buffer4 *ct_buffer;
    __u8 audited = 0;
    bool has_l4_header = false;
    bool __maybe_unused dst_remote_ep = false;
    enum ct_status ct_status;
    __u16 proxy_port = 0;
    bool from_l7lb = false;
    bool emit_policy_verdict = true;
    if (!revalidate_data (ctx, &data, &data_end, &ip4))
        return DROP_INVALID;
    has_l4_header = ipv4_has_l4_header (ip4);
    if (1) {
        struct remote_endpoint_info *info;
        info = lookup_ip4_remote_endpoint (ip4->daddr);
        if (info && info->sec_label) {
            *dst_id = info->sec_label;
            tunnel_endpoint = info->tunnel_endpoint;
            encrypt_key = get_min_encrypt_key (info->key);
#ifdef ENABLE_WIREGUARD
            if (info->tunnel_endpoint != 0 && !identity_is_node (info->sec_label))
                dst_remote_ep = true;
#endif /* ENABLE_WIREGUARD */
        }
        else {
            *dst_id = WORLD_ID;
        }
        cilium_dbg (ctx, info ? DBG_IP_ID_MAP_SUCCEED4 : DBG_IP_ID_MAP_FAILED4, ip4 -> daddr, * dst_id);
    }
#ifdef ENABLE_PER_PACKET_LB
    lb4_ctx_restore_state (ctx, & ct_state_new, ip4 -> daddr, & proxy_port);
    hairpin_flow = ct_state_new.loopback;
#endif /* ENABLE_PER_PACKET_LB */
    l4_off = ETH_HLEN + ipv4_hdrlen (ip4);
    ct_buffer = map_lookup_elem (&CT_TAIL_CALL_BUFFER4, &zero);
    if (!ct_buffer)
        return DROP_INVALID_TC_BUFFER;
    if (ct_buffer->tuple.saddr == 0)
        return DROP_INVALID_TC_BUFFER;
#if HAVE_DIRECT_ACCESS_TO_MAP_VALUES
    tuple = (struct ipv4_ct_tuple *) &ct_buffer->tuple;
    ct_state = (struct ct_state *) &ct_buffer->ct_state;
#else
    memcpy (& tuple_on_stack, & ct_buffer -> tuple, sizeof (tuple_on_stack));
    tuple = &tuple_on_stack;
    memcpy (& ct_state_on_stack, & ct_buffer -> ct_state, sizeof (ct_state_on_stack));
    ct_state = &ct_state_on_stack;
#endif /* HAVE_DIRECT_ACCESS_TO_MAP_VALUES */
    trace.monitor = ct_buffer->monitor;
    ret = ct_buffer->ret;
    ct_status = (enum ct_status) ret;
    trace.reason = (enum trace_reason) ret;
#if defined(ENABLE_L7_LB)
    if (proxy_port > 0) {
        cilium_dbg3 (ctx, DBG_L7_LB, tuple -> daddr, tuple -> saddr, bpf_ntohs (proxy_port));
        verdict = proxy_port;
        emit_policy_verdict = false;
        goto skip_policy_enforcement;
    }
#endif /* ENABLE_L7_LB */
    if ((ct_status == CT_REPLY || ct_status == CT_RELATED) && ct_state->proxy_redirect) {
        return ctx_redirect_to_proxy4 (ctx, tuple, 0, false);
    }
    if (hairpin_flow) {
        emit_policy_verdict = false;
        goto skip_policy_enforcement;
    }
    verdict = policy_can_egress4 (ctx, tuple, SECLABEL, *dst_id, &policy_match_type, &audited);
    if (ct_status != CT_REPLY && ct_status != CT_RELATED && verdict < 0) {
        send_policy_verdict_notify (ctx, * dst_id, tuple -> dport, tuple -> nexthdr, POLICY_EGRESS, 0, verdict, policy_match_type, audited);
        return verdict;
    }
skip_policy_enforcement :
#if defined(ENABLE_L7_LB)
    from_l7lb = ctx_load_meta (ctx, CB_FROM_HOST) == FROM_HOST_L7_LB;
#endif
    switch (ct_status) {
    case CT_NEW :
        if (emit_policy_verdict)
            send_policy_verdict_notify (ctx, *dst_id, tuple->dport, tuple->nexthdr, POLICY_EGRESS, 0, verdict, policy_match_type, audited);
    ct_recreate4 :
        ct_state_new.src_sec_id = SECLABEL;
        ret = ct_create4 (get_ct_map4 (tuple), &CT_MAP_ANY4, tuple, ctx, CT_EGRESS, &ct_state_new, verdict > 0, from_l7lb);
        if (IS_ERR (ret))
            return ret;
        break;
    case CT_REOPENED :
        if (emit_policy_verdict)
            send_policy_verdict_notify (ctx, *dst_id, tuple->dport, tuple->nexthdr, POLICY_EGRESS, 0, verdict, policy_match_type, audited);
    case CT_ESTABLISHED :
        if (unlikely (ct_state->rev_nat_index != ct_state_new.rev_nat_index))
            goto ct_recreate4;
        break;
    case CT_RELATED :
    case CT_REPLY :
        policy_mark_skip (ctx);
#ifdef ENABLE_NODEPORT
# ifdef ENABLE_DSR
        if (ct_state->dsr) {
            ret = xlate_dsr_v4 (ctx, tuple, l4_off, has_l4_header);
            if (ret != 0)
                return ret;
        }
        else
# endif /* ENABLE_DSR */
            if (ct_state->node_port) {
                send_trace_notify (ctx, TRACE_TO_NETWORK, SECLABEL, * dst_id, 0, 0, trace.reason, trace.monitor);
                ctx->tc_index |= TC_INDEX_F_SKIP_RECIRCULATION;
                ep_tail_call (ctx, CILIUM_CALL_IPV4_NODEPORT_REVNAT);
                return DROP_MISSED_TAIL_CALL;
            }
#endif /* ENABLE_NODEPORT */
        if (ct_state->rev_nat_index) {
            struct csum_offset csum_off = {};
            csum_l4_offset_and_flags (tuple -> nexthdr, & csum_off);
            ret = lb4_rev_nat (ctx, ETH_HLEN, l4_off, &csum_off, ct_state, tuple, 0, has_l4_header);
            if (IS_ERR (ret))
                return ret;
        }
        break;
    default :
        return DROP_UNKNOWN_CT;
    }
    hairpin_flow |= ct_state->loopback;
    if (!from_l7lb && redirect_to_proxy (verdict, ct_status)) {
        proxy_port = (__u16) verdict;
        send_trace_notify (ctx, TRACE_TO_PROXY, SECLABEL, 0, bpf_ntohs (proxy_port), 0, trace.reason, trace.monitor);
        return ctx_redirect_to_proxy4 (ctx, tuple, proxy_port, false);
    }
    if (!revalidate_data (ctx, &data, &data_end, &ip4))
        return DROP_INVALID;
    if (is_defined (ENABLE_ROUTING) || hairpin_flow) {
        struct endpoint_info *ep;
        ep = lookup_ip4_endpoint (ip4);
        if (ep) {
#ifdef ENABLE_ROUTING
            if (ep->flags & ENDPOINT_F_HOST) {
#ifdef HOST_IFINDEX
                goto to_host;
#else
                return DROP_HOST_UNREACHABLE;
#endif
            }
#endif /* ENABLE_ROUTING */
            policy_clear_mark (ctx);
            return ipv4_local_delivery (ctx, ETH_HLEN, SECLABEL, ip4, ep, METRIC_EGRESS, from_l7lb);
        }
    }
#if defined(ENABLE_HOST_FIREWALL) && !defined(ENABLE_ROUTING)
    if (*dst_id == HOST_ID) {
        ctx_store_meta (ctx, CB_FROM_HOST, 0);
        tail_call_static (ctx, & POLICY_CALL_MAP, HOST_EP_ID);
        return DROP_MISSED_TAIL_CALL;
    }
#endif /* ENABLE_HOST_FIREWALL && !ENABLE_ROUTING */
#ifdef ENABLE_EGRESS_GATEWAY
    {
        struct egress_gw_policy_entry *egress_gw_policy;
        struct endpoint_info *gateway_node_ep;
        struct endpoint_key key = {};
        if (identity_is_cluster (*dst_id))
            goto skip_egress_gateway;
        if (ct_status == CT_REPLY || ct_status == CT_RELATED)
            goto skip_egress_gateway;
        egress_gw_policy = lookup_ip4_egress_gw_policy (ip4->saddr, ip4->daddr);
        if (!egress_gw_policy)
            goto skip_egress_gateway;
        gateway_node_ep = __lookup_ip4_endpoint (egress_gw_policy->gateway_ip);
        if (gateway_node_ep && (gateway_node_ep->flags & ENDPOINT_F_HOST))
            goto skip_egress_gateway;
        ret = encap_and_redirect_lxc (ctx, egress_gw_policy->gateway_ip, encrypt_key, &key, SECLABEL, &trace);
        if (ret == IPSEC_ENDPOINT)
            goto encrypt_to_stack;
        else
            return ret;
    }
skip_egress_gateway :
#endif
#if defined(ENABLE_VTEP)
    {
        struct vtep_key vkey = {};
        struct vtep_value *vtep;
        vkey.vtep_ip = ip4->daddr & VTEP_MASK;
        vtep = map_lookup_elem (&VTEP_MAP, &vkey);
        if (!vtep)
            goto skip_vtep;
        if (vtep->vtep_mac && vtep->tunnel_endpoint) {
            if (eth_store_daddr (ctx, (__u8 *) &vtep->vtep_mac, 0) < 0)
                return DROP_WRITE_ERROR;
            return __encap_and_redirect_with_nodeid (ctx, vtep->tunnel_endpoint, SECLABEL, WORLD_ID, &trace);
        }
    }
skip_vtep :
#endif
#ifdef TUNNEL_MODE
# ifdef ENABLE_WIREGUARD
    if (!dst_remote_ep)
# endif /* ENABLE_WIREGUARD */
        {
            struct endpoint_key key = {};
            key.ip4 = ip4->daddr & IPV4_MASK;
            key.family = ENDPOINT_KEY_IPV4;
            ret = encap_and_redirect_lxc (ctx, tunnel_endpoint, encrypt_key, &key, SECLABEL, &trace);
            if (ret == DROP_NO_TUNNEL_ENDPOINT)
                goto pass_to_stack;
            else if (ret == IPSEC_ENDPOINT)
                goto encrypt_to_stack;
            else
                return ret;
        }
#endif /* TUNNEL_MODE */
    if (is_defined (ENABLE_HOST_ROUTING))
        return redirect_direct_v4 (ctx, ETH_HLEN, ip4);
    goto pass_to_stack;
#ifdef ENABLE_ROUTING
to_host :
    if (is_defined (ENABLE_HOST_FIREWALL) && *dst_id == HOST_ID) {
        send_trace_notify (ctx, TRACE_TO_HOST, SECLABEL, HOST_ID, 0, HOST_IFINDEX, trace.reason, trace.monitor);
        return ctx_redirect (ctx, HOST_IFINDEX, BPF_F_INGRESS);
    }
#endif
pass_to_stack :
#ifdef ENABLE_ROUTING
    ret = ipv4_l3 (ctx, ETH_HLEN, NULL, (__u8 *) &router_mac.addr, ip4);
    if (unlikely (ret != CTX_ACT_OK))
        return ret;
#endif
#ifdef ENABLE_WIREGUARD
    if (dst_remote_ep)
        set_encrypt_mark (ctx);
    else
#elif !defined(TUNNEL_MODE)
# ifdef ENABLE_IPSEC
        if (encrypt_key && tunnel_endpoint) {
            set_encrypt_key_mark (ctx, encrypt_key);
#  ifdef IP_POOLS
            set_encrypt_dip (ctx, tunnel_endpoint);
#  endif /* IP_POOLS */
#  ifdef ENABLE_IDENTITY_MARK
            set_identity_mark (ctx, SECLABEL);
#  endif
        }
        else
# endif /* ENABLE_IPSEC */
#endif /* ENABLE_WIREGUARD */
            {
#ifdef ENABLE_IDENTITY_MARK
                ctx->mark |= MARK_MAGIC_IDENTITY;
                set_identity_mark (ctx, SECLABEL);
#endif
            }
#if defined(TUNNEL_MODE) || defined(ENABLE_EGRESS_GATEWAY)
encrypt_to_stack :
#endif
    send_trace_notify (ctx, TRACE_TO_STACK, SECLABEL, * dst_id, 0, 0, trace.reason, trace.monitor);
    cilium_dbg_capture (ctx, DBG_CAPTURE_DELIVERY, 0);
    return CTX_ACT_OK;
}

declare_tailcall_if (is_defined (ENABLE_PER_PACKET_LB), CILIUM_CALL_IPV4_FROM_LXC_CONT)
int tail_handle_ipv4_cont (struct __ctx_buff *ctx) {
    __u32 dst_id = 0;
    int ret = handle_ipv4_from_lxc (ctx, &dst_id);
    if (IS_ERR (ret))
        return send_drop_notify (ctx, SECLABEL, dst_id, 0, ret, CTX_ACT_DROP, METRIC_EGRESS);
#ifdef ENABLE_CUSTOM_CALLS
    if (!encode_custom_prog_meta (ctx, ret, dst_id)) {
        tail_call_static (ctx, & CUSTOM_CALLS_MAP, CUSTOM_CALLS_IDX_IPV4_EGRESS);
        update_metrics (ctx_full_len (ctx), METRIC_EGRESS, REASON_MISSED_CUSTOM_CALL);
    }
#endif
    return ret;
}

TAIL_CT_LOOKUP4 (CILIUM_CALL_IPV4_CT_EGRESS, tail_ipv4_ct_egress, CT_EGRESS, is_defined (ENABLE_PER_PACKET_LB), CILIUM_CALL_IPV4_FROM_LXC_CONT, tail_handle_ipv4_cont)
static __always_inline int __tail_handle_ipv4 (struct __ctx_buff *ctx) {
    void *data, *data_end;
    struct iphdr *ip4;
    int ret;
    if (!revalidate_data_pull (ctx, &data, &data_end, &ip4))
        return DROP_INVALID;
#ifndef ENABLE_IPV4_FRAGMENTS
    if (ipv4_is_fragment (ip4))
        return DROP_FRAG_NOSUPPORT;
#endif
    if (unlikely (!is_valid_lxc_src_ipv4 (ip4)))
        return DROP_INVALID_SIP;
#ifdef ENABLE_PER_PACKET_LB
    {
        struct ipv4_ct_tuple tuple = {};
        struct csum_offset csum_off = {};
        struct ct_state ct_state_new = {};
        bool has_l4_header;
        struct lb4_service *svc;
        struct lb4_key key = {};
        __u16 proxy_port = 0;
        int l4_off;
        has_l4_header = ipv4_has_l4_header (ip4);
        tuple.nexthdr = ip4->protocol;
        tuple.daddr = ip4->daddr;
        tuple.saddr = ip4->saddr;
        l4_off = ETH_HLEN + ipv4_hdrlen (ip4);
        ret = lb4_extract_key (ctx, ip4, l4_off, &key, &csum_off, CT_EGRESS);
        if (IS_ERR (ret)) {
            if (ret == DROP_NO_SERVICE || ret == DROP_UNKNOWN_L4)
                goto skip_service_lookup;
            else
                return ret;
        }
        svc = lb4_lookup_service (&key, is_defined (ENABLE_NODEPORT));
        if (svc) {
#if defined(ENABLE_L7_LB)
            if (lb4_svc_is_l7loadbalancer (svc)) {
                proxy_port = (__u16) svc->l7_lb_proxy_port;
                goto skip_service_lookup;
            }
#endif /* ENABLE_L7_LB */
            ret = lb4_local (get_ct_map4 (&tuple), ctx, ETH_HLEN, l4_off, &csum_off, &key, &tuple, svc, &ct_state_new, ip4->saddr, has_l4_header, false);
            if (IS_ERR (ret))
                return ret;
        }
    skip_service_lookup :
        lb4_ctx_store_state (ctx, &ct_state_new, proxy_port);
    }
#endif /* ENABLE_PER_PACKET_LB */
    invoke_tailcall_if (is_defined (ENABLE_PER_PACKET_LB), CILIUM_CALL_IPV4_CT_EGRESS, tail_ipv4_ct_egress);
    return ret;
}

__section_tail (CILIUM_MAP_CALLS, CILIUM_CALL_IPV4_FROM_LXC)
int tail_handle_ipv4 (struct __ctx_buff *ctx) {
    int ret = __tail_handle_ipv4 (ctx);
    if (IS_ERR (ret))
        return send_drop_notify_error (ctx, SECLABEL, ret, CTX_ACT_DROP, METRIC_EGRESS);
    return ret;
}
#ifdef ENABLE_ARP_RESPONDER

__section_tail (CILIUM_MAP_CALLS, CILIUM_CALL_ARP)
int tail_handle_arp (struct __ctx_buff *ctx) {
    union macaddr mac = NODE_MAC;
    union macaddr smac;
    __be32 sip;
    __be32 tip;
    if (!arp_validate (ctx, &mac, &smac, &sip, &tip))
        return CTX_ACT_OK;
    if (tip == LXC_IPV4)
        return CTX_ACT_OK;
    return arp_respond (ctx, &mac, tip, &smac, sip, 0);
}
#endif /* ENABLE_ARP_RESPONDER */
#endif /* ENABLE_IPV4 */

__section ("from-container")
int handle_xgress (struct __ctx_buff *ctx) {
    __u16 proto;
    int ret;
    bpf_clear_meta (ctx);
    reset_queue_mapping (ctx);
    send_trace_notify (ctx, TRACE_FROM_LXC, SECLABEL, 0, 0, 0, TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
    if (!validate_ethertype (ctx, &proto)) {
        ret = DROP_UNSUPPORTED_L2;
        goto out;
    }
    switch (proto) {
#ifdef ENABLE_IPV6
    case bpf_htons (ETH_P_IPV6) :
        edt_set_aggregate (ctx, LXC_ID);
        ep_tail_call (ctx, CILIUM_CALL_IPV6_FROM_LXC);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#endif /* ENABLE_IPV6 */
#ifdef ENABLE_IPV4
    case bpf_htons (ETH_P_IP) :
        edt_set_aggregate (ctx, LXC_ID);
        ep_tail_call (ctx, CILIUM_CALL_IPV4_FROM_LXC);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#ifdef ENABLE_ARP_PASSTHROUGH
    case bpf_htons (ETH_P_ARP) :
        ret = CTX_ACT_OK;
        break;
#elif defined(ENABLE_ARP_RESPONDER)
    case bpf_htons (ETH_P_ARP) :
        ep_tail_call (ctx, CILIUM_CALL_ARP);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#endif /* ENABLE_ARP_RESPONDER */
#endif /* ENABLE_IPV4 */
    default :
        ret = DROP_UNKNOWN_L3;
    }
out :
    if (IS_ERR (ret))
        return send_drop_notify (ctx, SECLABEL, 0, 0, ret, CTX_ACT_DROP, METRIC_EGRESS);
    return ret;
}
#ifdef ENABLE_IPV6

static __always_inline int ipv6_policy (struct __ctx_buff *ctx, int ifindex, __u32 src_label, enum ct_status *ct_status, struct ipv6_ct_tuple *tuple_out, __u16 *proxy_port, bool from_host __maybe_unused) {
    struct ct_state ct_state_on_stack __maybe_unused, *ct_state, ct_state_new = {};
    struct ipv6_ct_tuple tuple_on_stack __maybe_unused, *tuple;
    int ret, verdict, hdrlen, zero = 0;
    struct ct_buffer6 *ct_buffer;
    void *data, *data_end;
    struct ipv6hdr *ip6;
    bool skip_ingress_proxy = false;
    enum trace_reason reason;
    union v6addr orig_sip;
    __u32 monitor = 0;
    __u8 policy_match_type = POLICY_MATCH_NONE;
    __u8 audited = 0;
    bool emit_policy_verdict = true;
    if (!revalidate_data (ctx, &data, &data_end, &ip6))
        return DROP_INVALID;
    policy_clear_mark (ctx);
    ipv6_addr_copy (& orig_sip, (union v6addr *) & ip6 -> saddr);
    skip_ingress_proxy = tc_index_skip_ingress_proxy (ctx);
    ct_buffer = map_lookup_elem (&CT_TAIL_CALL_BUFFER6, &zero);
    if (!ct_buffer)
        return DROP_INVALID_TC_BUFFER;
    if (ct_buffer->tuple.saddr.d1 == 0 && ct_buffer->tuple.saddr.d2 == 0)
        return DROP_INVALID_TC_BUFFER;
#if HAVE_DIRECT_ACCESS_TO_MAP_VALUES
    tuple = (struct ipv6_ct_tuple *) &ct_buffer->tuple;
    ct_state = (struct ct_state *) &ct_buffer->ct_state;
#else
    memcpy (& tuple_on_stack, & ct_buffer -> tuple, sizeof (tuple_on_stack));
    tuple = &tuple_on_stack;
    memcpy (& ct_state_on_stack, & ct_buffer -> ct_state, sizeof (ct_state_on_stack));
    ct_state = &ct_state_on_stack;
#endif /* HAVE_DIRECT_ACCESS_TO_MAP_VALUES */
    monitor = ct_buffer->monitor;
    ret = ct_buffer->ret;
    *ct_status = (enum ct_status) ret;
    if ((ret == CT_REPLY || ret == CT_RELATED) && (ct_state_is_from_l7lb (ct_state) || (ct_state->proxy_redirect && !tc_index_skip_egress_proxy (ctx)))) {
        send_trace_notify6 (ctx, TRACE_TO_PROXY, src_label, SECLABEL, & orig_sip, 0, ifindex, (enum trace_reason) ret, monitor);
        if (tuple_out)
            memcpy (tuple_out, tuple, sizeof (*tuple));
        return POLICY_ACT_PROXY_REDIRECT;
    }
    if (unlikely (ct_state->rev_nat_index)) {
        struct csum_offset csum_off = {};
        int ret2, l4_off;
        hdrlen = ipv6_hdrlen (ctx, &tuple->nexthdr);
        if (hdrlen < 0)
            return hdrlen;
        l4_off = ETH_HLEN + hdrlen;
        csum_l4_offset_and_flags (tuple -> nexthdr, & csum_off);
        ret2 = lb6_rev_nat (ctx, l4_off, &csum_off, ct_state->rev_nat_index, tuple, 0);
        if (IS_ERR (ret2))
            return ret2;
    }
    verdict = policy_can_access_ingress (ctx, src_label, SECLABEL, tuple->dport, tuple->nexthdr, false, &policy_match_type, &audited);
    if (ret != CT_REPLY && ret != CT_RELATED && verdict < 0) {
        send_policy_verdict_notify (ctx, src_label, tuple -> dport, tuple -> nexthdr, POLICY_INGRESS, 1, verdict, policy_match_type, audited);
        return verdict;
    }
    if (skip_ingress_proxy) {
        verdict = 0;
        emit_policy_verdict = false;
    }
    if (emit_policy_verdict && (ret == CT_NEW || ret == CT_REOPENED)) {
        send_policy_verdict_notify (ctx, src_label, tuple -> dport, tuple -> nexthdr, POLICY_INGRESS, 1, verdict, policy_match_type, audited);
    }
#ifdef ENABLE_NODEPORT
    if (ret == CT_NEW || ret == CT_REOPENED) {
        bool dsr = false;
# ifdef ENABLE_DSR
        int ret2;
        ret2 = handle_dsr_v6 (ctx, &dsr);
        if (ret2 != 0)
            return ret2;
        ct_state_new.dsr = dsr;
        if (ret == CT_REOPENED && ct_state->dsr != dsr)
            ct_update6_dsr (get_ct_map6 (tuple), tuple, dsr);
# endif /* ENABLE_DSR */
        if (!dsr) {
            bool node_port = ct_has_nodeport_egress_entry6 (get_ct_map6 (tuple), tuple);
            ct_state_new.node_port = node_port;
            if (ret == CT_REOPENED && ct_state->node_port != node_port)
                ct_update_nodeport (get_ct_map6 (tuple), tuple, node_port);
        }
    }
#endif /* ENABLE_NODEPORT */
    if (ret == CT_NEW) {
        ct_state_new.src_sec_id = src_label;
        ret = ct_create6 (get_ct_map6 (tuple), &CT_MAP_ANY6, tuple, ctx, CT_INGRESS, &ct_state_new, verdict > 0, false);
        if (IS_ERR (ret))
            return ret;
    }
    if (!revalidate_data (ctx, &data, &data_end, &ip6))
        return DROP_INVALID;
    reason = (enum trace_reason) *ct_status;
    if (redirect_to_proxy (verdict, *ct_status)) {
        *proxy_port = (__u16) verdict;
        send_trace_notify6 (ctx, TRACE_TO_PROXY, src_label, SECLABEL, & orig_sip, bpf_ntohs (* proxy_port), ifindex, reason, monitor);
        if (tuple_out)
            memcpy (tuple_out, tuple, sizeof (*tuple));
        return POLICY_ACT_PROXY_REDIRECT;
    }
    send_trace_notify6 (ctx, TRACE_TO_LXC, src_label, SECLABEL, & orig_sip, LXC_ID, ifindex, reason, monitor);
#if !defined(ENABLE_ROUTING) && defined(TUNNEL_MODE) && !defined(ENABLE_NODEPORT)
    ctx_change_type (ctx, PACKET_HOST);
#else
    ifindex = ctx_load_meta (ctx, CB_IFINDEX);
    if (ifindex)
        return redirect_ep (ctx, ifindex, from_host);
#endif /* !ENABLE_ROUTING && TUNNEL_MODE && !ENABLE_NODEPORT */
    return CTX_ACT_OK;
}

declare_tailcall_if (__and (is_defined (ENABLE_IPV4), is_defined (ENABLE_IPV6)), CILIUM_CALL_IPV6_TO_LXC_POLICY_ONLY)
int tail_ipv6_policy (struct __ctx_buff *ctx) {
    struct ipv6_ct_tuple tuple = {};
    int ret, ifindex = ctx_load_meta (ctx, CB_IFINDEX);
    __u32 src_label = ctx_load_meta (ctx, CB_SRC_LABEL);
    bool from_host = ctx_load_meta (ctx, CB_FROM_HOST);
    bool proxy_redirect __maybe_unused = false;
    __u16 proxy_port = 0;
    enum ct_status ct_status = 0;
    ctx_store_meta (ctx, CB_SRC_LABEL, 0);
    ctx_store_meta (ctx, CB_FROM_HOST, 0);
    ret = ipv6_policy (ctx, ifindex, src_label, &ct_status, &tuple, &proxy_port, from_host);
    if (ret == POLICY_ACT_PROXY_REDIRECT) {
        ret = ctx_redirect_to_proxy6 (ctx, &tuple, proxy_port, from_host);
        proxy_redirect = true;
    }
    if (IS_ERR (ret))
        return send_drop_notify (ctx, src_label, SECLABEL, LXC_ID, ret, CTX_ACT_DROP, METRIC_INGRESS);
    ctx_store_meta (ctx, CB_PROXY_MAGIC, ctx -> mark);
#ifdef ENABLE_CUSTOM_CALLS
    if (!proxy_redirect && !encode_custom_prog_meta (ctx, ret, src_label)) {
        tail_call_static (ctx, & CUSTOM_CALLS_MAP, CUSTOM_CALLS_IDX_IPV6_INGRESS);
        update_metrics (ctx_full_len (ctx), METRIC_INGRESS, REASON_MISSED_CUSTOM_CALL);
    }
#endif
    return ret;
}

__section_tail (CILIUM_MAP_CALLS, CILIUM_CALL_IPV6_TO_ENDPOINT)
int tail_ipv6_to_endpoint (struct __ctx_buff *ctx) {
    __u32 src_identity = ctx_load_meta (ctx, CB_SRC_LABEL);
    bool proxy_redirect __maybe_unused = false;
    void *data, *data_end;
    struct ipv6hdr *ip6;
    __u16 proxy_port = 0;
    enum ct_status ct_status;
    int ret;
    if (!revalidate_data (ctx, &data, &data_end, &ip6)) {
        ret = DROP_INVALID;
        goto out;
    }
    if (identity_is_reserved (src_identity)) {
        union v6addr *src = (union v6addr *) &ip6->saddr;
        struct remote_endpoint_info *info;
        info = lookup_ip6_remote_endpoint (src);
        if (info != NULL) {
            __u32 sec_label = info->sec_label;
            if (sec_label) {
                if (sec_label != HOST_ID)
                    src_identity = sec_label;
            }
        }
        cilium_dbg (ctx, info ? DBG_IP_ID_MAP_SUCCEED6 : DBG_IP_ID_MAP_FAILED6, ((__u32 *) src) [3], src_identity);
    }
    cilium_dbg (ctx, DBG_LOCAL_DELIVERY, LXC_ID, SECLABEL);
#ifdef LOCAL_DELIVERY_METRICS
    update_metrics (ctx_full_len (ctx), METRIC_INGRESS, REASON_FORWARDED);
#endif
    ctx_store_meta (ctx, CB_SRC_LABEL, 0);
    ret = ipv6_policy (ctx, 0, src_identity, &ct_status, NULL, &proxy_port, true);
    if (ret == POLICY_ACT_PROXY_REDIRECT) {
        ret = ctx_redirect_to_proxy_hairpin_ipv6 (ctx, proxy_port);
        proxy_redirect = true;
    }
out :
    if (IS_ERR (ret))
        return send_drop_notify (ctx, src_identity, SECLABEL, LXC_ID, ret, CTX_ACT_DROP, METRIC_INGRESS);
#ifdef ENABLE_CUSTOM_CALLS
    if (!proxy_redirect && !encode_custom_prog_meta (ctx, ret, src_identity)) {
        tail_call_static (ctx, & CUSTOM_CALLS_MAP, CUSTOM_CALLS_IDX_IPV6_INGRESS);
        update_metrics (ctx_full_len (ctx), METRIC_INGRESS, REASON_MISSED_CUSTOM_CALL);
    }
#endif
    return ret;
}

TAIL_CT_LOOKUP6 (CILIUM_CALL_IPV6_CT_INGRESS_POLICY_ONLY, tail_ipv6_ct_ingress_policy_only, CT_INGRESS, __and (is_defined (ENABLE_IPV4), is_defined (ENABLE_IPV6)), CILIUM_CALL_IPV6_TO_LXC_POLICY_ONLY, tail_ipv6_policy)
TAIL_CT_LOOKUP6 (CILIUM_CALL_IPV6_CT_INGRESS, tail_ipv6_ct_ingress, CT_INGRESS, 1, CILIUM_CALL_IPV6_TO_ENDPOINT, tail_ipv6_to_endpoint)
#endif /* ENABLE_IPV6 */
#ifdef ENABLE_IPV4

static __always_inline int ipv4_policy (struct __ctx_buff *ctx, int ifindex, __u32 src_label, enum ct_status *ct_status, struct ipv4_ct_tuple *tuple_out, __u16 *proxy_port, bool from_host __maybe_unused) {
    struct ct_state ct_state_on_stack __maybe_unused, *ct_state, ct_state_new = {};
    struct ipv4_ct_tuple tuple_on_stack __maybe_unused, *tuple;
    void *data, *data_end;
    struct iphdr *ip4;
    bool skip_ingress_proxy = false;
    bool is_untracked_fragment = false;
    struct ct_buffer4 *ct_buffer;
    __u32 monitor = 0, zero = 0;
    enum trace_reason reason;
    int ret, verdict = 0;
    __be32 orig_sip;
    __u8 policy_match_type = POLICY_MATCH_NONE;
    __u8 audited = 0;
    bool emit_policy_verdict = true;
    if (!revalidate_data (ctx, &data, &data_end, &ip4))
        return DROP_INVALID;
    policy_clear_mark (ctx);
    skip_ingress_proxy = tc_index_skip_ingress_proxy (ctx);
    orig_sip = ip4->saddr;
#ifndef ENABLE_IPV4_FRAGMENTS
    is_untracked_fragment = ipv4_is_fragment (ip4);
#endif
    ct_buffer = map_lookup_elem (&CT_TAIL_CALL_BUFFER4, &zero);
    if (!ct_buffer)
        return DROP_INVALID_TC_BUFFER;
    if (ct_buffer->tuple.saddr == 0)
        return DROP_INVALID_TC_BUFFER;
#if HAVE_DIRECT_ACCESS_TO_MAP_VALUES
    tuple = (struct ipv4_ct_tuple *) &ct_buffer->tuple;
    ct_state = (struct ct_state *) &ct_buffer->ct_state;
#else
    memcpy (& tuple_on_stack, & ct_buffer -> tuple, sizeof (tuple_on_stack));
    tuple = &tuple_on_stack;
    memcpy (& ct_state_on_stack, & ct_buffer -> ct_state, sizeof (ct_state_on_stack));
    ct_state = &ct_state_on_stack;
#endif /* HAVE_DIRECT_ACCESS_TO_MAP_VALUES */
    monitor = ct_buffer->monitor;
    ret = ct_buffer->ret;
    *ct_status = (enum ct_status) ret;
    relax_verifier ();
    if ((ret == CT_REPLY || ret == CT_RELATED) && (ct_state_is_from_l7lb (ct_state) || (ct_state->proxy_redirect && !tc_index_skip_egress_proxy (ctx)))) {
        send_trace_notify4 (ctx, TRACE_TO_PROXY, src_label, SECLABEL, orig_sip, 0, ifindex, (enum trace_reason) ret, monitor);
        if (tuple_out)
            *tuple_out = *tuple;
        return POLICY_ACT_PROXY_REDIRECT;
    }
    if (unlikely (ret == CT_REPLY && ct_state->rev_nat_index && !ct_state->loopback)) {
        struct csum_offset csum_off = {};
        bool has_l4_header = false;
        int ret2, l4_off;
        l4_off = ETH_HLEN + ipv4_hdrlen (ip4);
        has_l4_header = ipv4_has_l4_header (ip4);
        if (has_l4_header)
            csum_l4_offset_and_flags (tuple->nexthdr, &csum_off);
        ret2 = lb4_rev_nat (ctx, ETH_HLEN, l4_off, &csum_off, ct_state, tuple, REV_NAT_F_TUPLE_SADDR, has_l4_header);
        if (IS_ERR (ret2))
            return ret2;
    }
#if defined(ENABLE_PER_PACKET_LB) && !defined(DISABLE_LOOPBACK_LB)
    if (unlikely (ct_state->loopback))
        goto skip_policy_enforcement;
#endif /* ENABLE_PER_PACKET_LB && !DISABLE_LOOPBACK_LB */
    verdict = policy_can_access_ingress (ctx, src_label, SECLABEL, tuple->dport, tuple->nexthdr, is_untracked_fragment, &policy_match_type, &audited);
    if (ret != CT_REPLY && ret != CT_RELATED && verdict < 0) {
        send_policy_verdict_notify (ctx, src_label, tuple -> dport, tuple -> nexthdr, POLICY_INGRESS, 0, verdict, policy_match_type, audited);
        return verdict;
    }
    if (skip_ingress_proxy) {
        verdict = 0;
        emit_policy_verdict = false;
    }
    if (emit_policy_verdict && (ret == CT_NEW || ret == CT_REOPENED)) {
        send_policy_verdict_notify (ctx, src_label, tuple -> dport, tuple -> nexthdr, POLICY_INGRESS, 0, verdict, policy_match_type, audited);
    }
#if defined(ENABLE_PER_PACKET_LB) && !defined(DISABLE_LOOPBACK_LB)
skip_policy_enforcement :
#endif /* ENABLE_PER_PACKET_LB && !DISABLE_LOOPBACK_LB */
#ifdef ENABLE_NODEPORT
    if (ret == CT_NEW || ret == CT_REOPENED) {
        bool dsr = false;
# ifdef ENABLE_DSR
        int ret2;
        ret2 = handle_dsr_v4 (ctx, &dsr);
        if (ret2 != 0)
            return ret2;
        ct_state_new.dsr = dsr;
        if (ret == CT_REOPENED && ct_state->dsr != dsr)
            ct_update4_dsr (get_ct_map4 (tuple), tuple, dsr);
# endif /* ENABLE_DSR */
        if (!dsr) {
            bool node_port = ct_has_nodeport_egress_entry4 (get_ct_map4 (tuple), tuple);
            ct_state_new.node_port = node_port;
            if (ret == CT_REOPENED && ct_state->node_port != node_port)
                ct_update_nodeport (get_ct_map4 (tuple), tuple, node_port);
        }
    }
#endif /* ENABLE_NODEPORT */
    if (ret == CT_NEW) {
        ct_state_new.src_sec_id = src_label;
        ret = ct_create4 (get_ct_map4 (tuple), &CT_MAP_ANY4, tuple, ctx, CT_INGRESS, &ct_state_new, verdict > 0, false);
        if (IS_ERR (ret))
            return ret;
    }
    if (!revalidate_data (ctx, &data, &data_end, &ip4))
        return DROP_INVALID;
    reason = (enum trace_reason) *ct_status;
    if (redirect_to_proxy (verdict, *ct_status)) {
        *proxy_port = (__u16) verdict;
        send_trace_notify4 (ctx, TRACE_TO_PROXY, src_label, SECLABEL, orig_sip, bpf_ntohs (* proxy_port), ifindex, reason, monitor);
        if (tuple_out)
            *tuple_out = *tuple;
        return POLICY_ACT_PROXY_REDIRECT;
    }
    send_trace_notify4 (ctx, TRACE_TO_LXC, src_label, SECLABEL, orig_sip, LXC_ID, ifindex, reason, monitor);
#if !defined(ENABLE_ROUTING) && defined(TUNNEL_MODE) && !defined(ENABLE_NODEPORT)
    ctx_change_type (ctx, PACKET_HOST);
#else
    ifindex = ctx_load_meta (ctx, CB_IFINDEX);
    if (ifindex)
        return redirect_ep (ctx, ifindex, from_host);
#endif /* !ENABLE_ROUTING && TUNNEL_MODE && !ENABLE_NODEPORT */
    return CTX_ACT_OK;
}

declare_tailcall_if (__and (is_defined (ENABLE_IPV4), is_defined (ENABLE_IPV6)), CILIUM_CALL_IPV4_TO_LXC_POLICY_ONLY)
int tail_ipv4_policy (struct __ctx_buff *ctx) {
    struct ipv4_ct_tuple tuple = {};
    int ret, ifindex = ctx_load_meta (ctx, CB_IFINDEX);
    __u32 src_label = ctx_load_meta (ctx, CB_SRC_LABEL);
    bool from_host = ctx_load_meta (ctx, CB_FROM_HOST);
    bool proxy_redirect __maybe_unused = false;
    enum ct_status ct_status = 0;
    __u16 proxy_port = 0;
    ctx_store_meta (ctx, CB_SRC_LABEL, 0);
    ctx_store_meta (ctx, CB_FROM_HOST, 0);
    ret = ipv4_policy (ctx, ifindex, src_label, &ct_status, &tuple, &proxy_port, from_host);
    if (ret == POLICY_ACT_PROXY_REDIRECT) {
        ret = ctx_redirect_to_proxy4 (ctx, &tuple, proxy_port, from_host);
        proxy_redirect = true;
    }
    if (IS_ERR (ret))
        return send_drop_notify (ctx, src_label, SECLABEL, LXC_ID, ret, CTX_ACT_DROP, METRIC_INGRESS);
    ctx_store_meta (ctx, CB_PROXY_MAGIC, ctx -> mark);
#ifdef ENABLE_CUSTOM_CALLS
    if (!proxy_redirect && !encode_custom_prog_meta (ctx, ret, src_label)) {
        tail_call_static (ctx, & CUSTOM_CALLS_MAP, CUSTOM_CALLS_IDX_IPV4_INGRESS);
        update_metrics (ctx_full_len (ctx), METRIC_INGRESS, REASON_MISSED_CUSTOM_CALL);
    }
#endif
    return ret;
}

__section_tail (CILIUM_MAP_CALLS, CILIUM_CALL_IPV4_TO_ENDPOINT)
int tail_ipv4_to_endpoint (struct __ctx_buff *ctx) {
    __u32 src_identity = ctx_load_meta (ctx, CB_SRC_LABEL);
    bool proxy_redirect __maybe_unused = false;
    void *data, *data_end;
    struct iphdr *ip4;
    __u16 proxy_port = 0;
    enum ct_status ct_status;
    int ret;
    if (!revalidate_data (ctx, &data, &data_end, &ip4)) {
        ret = DROP_INVALID;
        goto out;
    }
    if (identity_is_reserved (src_identity)) {
        struct remote_endpoint_info *info;
        info = lookup_ip4_remote_endpoint (ip4->saddr);
        if (info != NULL) {
            __u32 sec_label = info->sec_label;
            if (sec_label) {
                if (sec_label != HOST_ID)
                    src_identity = sec_label;
            }
        }
        cilium_dbg (ctx, info ? DBG_IP_ID_MAP_SUCCEED4 : DBG_IP_ID_MAP_FAILED4, ip4 -> saddr, src_identity);
    }
    cilium_dbg (ctx, DBG_LOCAL_DELIVERY, LXC_ID, SECLABEL);
#ifdef LOCAL_DELIVERY_METRICS
    update_metrics (ctx_full_len (ctx), METRIC_INGRESS, REASON_FORWARDED);
#endif
    ctx_store_meta (ctx, CB_SRC_LABEL, 0);
    ret = ipv4_policy (ctx, 0, src_identity, &ct_status, NULL, &proxy_port, true);
    if (ret == POLICY_ACT_PROXY_REDIRECT) {
        ret = ctx_redirect_to_proxy_hairpin_ipv4 (ctx, proxy_port);
        proxy_redirect = true;
    }
out :
    if (IS_ERR (ret))
        return send_drop_notify (ctx, src_identity, SECLABEL, LXC_ID, ret, CTX_ACT_DROP, METRIC_INGRESS);
#ifdef ENABLE_CUSTOM_CALLS
    if (!proxy_redirect && !encode_custom_prog_meta (ctx, ret, src_identity)) {
        tail_call_static (ctx, & CUSTOM_CALLS_MAP, CUSTOM_CALLS_IDX_IPV4_INGRESS);
        update_metrics (ctx_full_len (ctx), METRIC_INGRESS, REASON_MISSED_CUSTOM_CALL);
    }
#endif
    return ret;
}

TAIL_CT_LOOKUP4 (CILIUM_CALL_IPV4_CT_INGRESS_POLICY_ONLY, tail_ipv4_ct_ingress_policy_only, CT_INGRESS, __and (is_defined (ENABLE_IPV4), is_defined (ENABLE_IPV6)), CILIUM_CALL_IPV4_TO_LXC_POLICY_ONLY, tail_ipv4_policy)
TAIL_CT_LOOKUP4 (CILIUM_CALL_IPV4_CT_INGRESS, tail_ipv4_ct_ingress, CT_INGRESS, 1, CILIUM_CALL_IPV4_TO_ENDPOINT, tail_ipv4_to_endpoint)
#endif /* ENABLE_IPV4 */

__section_tail (CILIUM_MAP_POLICY, TEMPLATE_LXC_ID)
int handle_policy (struct __ctx_buff *ctx) {
    __u32 src_label = ctx_load_meta (ctx, CB_SRC_LABEL);
    __u16 proto;
    int ret;
    if (!validate_ethertype (ctx, &proto)) {
        ret = DROP_UNSUPPORTED_L2;
        goto out;
    }
    switch (proto) {
#ifdef ENABLE_IPV6
    case bpf_htons (ETH_P_IPV6) :
        invoke_tailcall_if (__and (is_defined (ENABLE_IPV4), is_defined (ENABLE_IPV6)), CILIUM_CALL_IPV6_CT_INGRESS_POLICY_ONLY, tail_ipv6_ct_ingress_policy_only);
        break;
#endif /* ENABLE_IPV6 */
#ifdef ENABLE_IPV4
    case bpf_htons (ETH_P_IP) :
        invoke_tailcall_if (__and (is_defined (ENABLE_IPV4), is_defined (ENABLE_IPV6)), CILIUM_CALL_IPV4_CT_INGRESS_POLICY_ONLY, tail_ipv4_ct_ingress_policy_only);
        break;
#endif /* ENABLE_IPV4 */
    default :
        ret = DROP_UNKNOWN_L3;
        break;
    }
out :
    if (IS_ERR (ret))
        return send_drop_notify (ctx, src_label, SECLABEL, LXC_ID, ret, CTX_ACT_DROP, METRIC_INGRESS);
    return ret;
}
#if defined(ENABLE_L7_LB)

__section_tail (CILIUM_MAP_EGRESSPOLICY, TEMPLATE_LXC_ID)
int handle_policy_egress (struct __ctx_buff *ctx) {
    __u16 proto;
    int ret;
    if (!validate_ethertype (ctx, &proto)) {
        ret = DROP_UNSUPPORTED_L2;
        goto out;
    }
    ctx_store_meta (ctx, CB_FROM_HOST, FROM_HOST_L7_LB);
    edt_set_aggregate (ctx, 0);
    send_trace_notify (ctx, TRACE_FROM_PROXY, SECLABEL, 0, 0, 0, TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
    switch (proto) {
#ifdef ENABLE_IPV6
    case bpf_htons (ETH_P_IPV6) :
        ep_tail_call (ctx, CILIUM_CALL_IPV6_FROM_LXC);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#endif /* ENABLE_IPV6 */
#ifdef ENABLE_IPV4
    case bpf_htons (ETH_P_IP) :
        ep_tail_call (ctx, CILIUM_CALL_IPV4_FROM_LXC);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#endif /* ENABLE_IPV4 */
    default :
        ret = DROP_UNKNOWN_L3;
        break;
    }
out :
    if (IS_ERR (ret))
        return send_drop_notify (ctx, SECLABEL, 0, LXC_ID, ret, CTX_ACT_DROP, METRIC_EGRESS);
    return ret;
}
#endif

__section ("to-container")
int handle_to_container (struct __ctx_buff *ctx) {
    enum trace_point trace = TRACE_FROM_STACK;
    __u32 magic, identity = 0;
    __u16 proto;
    int ret;
    if (!validate_ethertype (ctx, &proto)) {
        ret = DROP_UNSUPPORTED_L2;
        goto out;
    }
    bpf_clear_meta (ctx);
    magic = inherit_identity_from_host (ctx, &identity);
    if (magic == MARK_MAGIC_PROXY_INGRESS || magic == MARK_MAGIC_PROXY_EGRESS)
        trace = TRACE_FROM_PROXY;
#if defined(ENABLE_L7_LB)
    else if (magic == MARK_MAGIC_PROXY_EGRESS_EPID) {
        tail_call_dynamic (ctx, & POLICY_EGRESSCALL_MAP, identity);
        return DROP_MISSED_TAIL_CALL;
    }
#endif
    send_trace_notify (ctx, trace, identity, 0, 0, ctx -> ingress_ifindex, TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
#if defined(ENABLE_HOST_FIREWALL) && !defined(ENABLE_ROUTING)
    if (identity == HOST_ID) {
        ctx_store_meta (ctx, CB_FROM_HOST, 1);
        ctx_store_meta (ctx, CB_DST_ENDPOINT_ID, LXC_ID);
        tail_call_static (ctx, & POLICY_CALL_MAP, HOST_EP_ID);
        return DROP_MISSED_TAIL_CALL;
    }
#endif /* ENABLE_HOST_FIREWALL && !ENABLE_ROUTING */
    ctx_store_meta (ctx, CB_SRC_LABEL, identity);
    switch (proto) {
#if defined(ENABLE_ARP_PASSTHROUGH) || defined(ENABLE_ARP_RESPONDER)
    case bpf_htons (ETH_P_ARP) :
        ret = CTX_ACT_OK;
        break;
#endif
#ifdef ENABLE_IPV6
    case bpf_htons (ETH_P_IPV6) :
        ep_tail_call (ctx, CILIUM_CALL_IPV6_CT_INGRESS);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#endif /* ENABLE_IPV6 */
#ifdef ENABLE_IPV4
    case bpf_htons (ETH_P_IP) :
        ep_tail_call (ctx, CILIUM_CALL_IPV4_CT_INGRESS);
        ret = DROP_MISSED_TAIL_CALL;
        break;
#endif /* ENABLE_IPV4 */
    default :
        ret = DROP_UNKNOWN_L3;
        break;
    }
out :
    if (IS_ERR (ret))
        return send_drop_notify (ctx, identity, SECLABEL, LXC_ID, ret, CTX_ACT_DROP, METRIC_INGRESS);
    return ret;
}

BPF_LICENSE ("Dual BSD/GPL");
